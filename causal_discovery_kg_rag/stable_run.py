#!/usr/bin/env python3
"""
é€šç”¨ç¨³å®šç‰ˆè¿è¡Œè„šæœ¬ï¼šæ”¯æŒå¤šç§æ•°æ®é›†
"""

import argparse
import logging
import time
import traceback
from pathlib import Path

import pandas as pd
from tqdm import tqdm

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent))

from experiment_logger import ExperimentLogger
from utils import extract_premise, extract_hypothesis
from pipeline.pipeline import CausalDiscoveryPipeline
from pipeline.stages import UndirectedSkeletonStage, VStructuresStage, MeekRulesStage, HypothesisEvaluationStage, BroadRetrievalStage, InitialConstructStage,LLMDAGComplementStage
from llm_client import OpenAIClient
from search_client import DuckDuckGoSearchClient
from kg_client import CNKnowledgeGraphClient
from pipeline.stages import KnowledgeGraphRetrievalStage
LOGS_DIR: Path = Path("logs/new")



from rag_client import RAGClient
from pipeline.stages import RAGEnhancementStage
def extract_proofwriter_format(input_text: str):
    """ä¸“é—¨å¤„ç†ProofWriteræ ¼å¼çš„æå–"""
    try:
        # ProofWriteræ•°æ®æ˜¯JSONæ ¼å¼
        import json
        import ast
        
        # å°è¯•è§£æä¸ºå­—å…¸
        if input_text.startswith("{") and input_text.endswith("}"):
            try:
                data = ast.literal_eval(input_text)
                if 'en' in data:
                    en_text = data['en']
                    # è§£æenå­—æ®µ: $answer$ ; $proof$ ; $question$ = [é—®é¢˜] ; $context$ = [ä¸Šä¸‹æ–‡]
                    if '$question$' in en_text and '$context$' in en_text:
                        parts = en_text.split('$context$ = ', 1)
                        if len(parts) == 2:
                            # ä»ç¬¬ä¸€éƒ¨åˆ†æå–é—®é¢˜
                            question_part = parts[0]
                            if '$question$ = ' in question_part:
                                question = question_part.split('$question$ = ')[-1].strip(' ;')
                            else:
                                question = ""
                            
                            # ç¬¬äºŒéƒ¨åˆ†æ˜¯ä¸Šä¸‹æ–‡
                            context = parts[1].strip()
                            
                            return context, question
            except:
                pass
        
        # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥æ–‡æœ¬è§£æ
        if '$question$' in input_text and '$context$' in input_text:
            parts = input_text.split('$context$ = ', 1)
            if len(parts) == 2:
                question_part = parts[0]
                if '$question$ = ' in question_part:
                    question = question_part.split('$question$ = ')[-1].strip(' ;')
                else:
                    question = ""
                context = parts[1].strip()
                return context, question
                
    except Exception as e:
        pass
    
    raise ValueError("Not ProofWriter format")

def safe_extract_premise_hypothesis(input_text: str, sample_id):
    """å®‰å…¨åœ°æå–premiseå’Œhypothesisï¼Œé’ˆå¯¹ä¸åŒæ•°æ®é›†æ ¼å¼ä¼˜åŒ–"""
    try:
        # å…ˆå°è¯•åŸæœ‰çš„æå–æ–¹å¼
        premise = extract_premise(input_text)
        hypothesis = extract_hypothesis(input_text) 
        logging.debug(f"Sample {sample_id}: Successfully extracted using standard method")
        return premise, hypothesis
    except Exception as e:
        logging.warning(f"Sample {sample_id}: Could not extract premise/hypothesis using standard method: {e}")
        
        # ä¸“é—¨å¤„ç†ProofWriteræ ¼å¼
        try:
            premise, hypothesis = extract_proofwriter_format(input_text)
            if premise or hypothesis:
                logging.info(f"Sample {sample_id}: Successfully extracted using ProofWriter format")
                return premise, hypothesis
        except Exception as pw_e:
            logging.debug(f"Sample {sample_id}: ProofWriter extraction failed: {pw_e}")
        
        # å…¶ä»–å¤‡ç”¨ç­–ç•¥...
        # (ä¿æŒä½ ç°æœ‰çš„å¤‡ç”¨ç­–ç•¥)
        
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        logging.warning(f"Sample {sample_id}: All extraction methods failed, using full text as premise")
        return input_text, ""

def clean_result_for_csv(result_data: dict) -> dict:
    """æ¸…ç†ç»“æœæ•°æ®ï¼Œåªä¿ç•™CSVéœ€è¦çš„å­—æ®µ"""
    # å®šä¹‰CSVéœ€è¦çš„å­—æ®µ
    csv_fields = {
        'sample_id', 'premise', 'hypothesis', 'sample_label', 
        'nodes', 'undirected_edges', 'v_structures', 'directed_edges',
        'hypothesis_label', 'enhanced_premise'
    }
    
    # åªä¿ç•™éœ€è¦çš„å­—æ®µ
    cleaned_result = {}
    for field in csv_fields:
        if field in result_data:
            cleaned_result[field] = result_data[field]
        else:
            # ä¸ºç¼ºå¤±çš„å­—æ®µè®¾ç½®é»˜è®¤å€¼
            cleaned_result[field] = None
    
    return cleaned_result

def safe_process_sample(pipeline, sample, max_retries=2):
    """å®‰å…¨åœ°å¤„ç†å•ä¸ªæ ·æœ¬ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
    sample_id = sample["sample_id"]
    
    for attempt in range(max_retries + 1):
        # try:
        logging.info(f"Processing sample {sample_id} (attempt {attempt + 1}/{max_retries + 1})")
        result = pipeline.run(sample)

        # ğŸ¯ å…³é”®ä¿®æ”¹ï¼šæ¸…ç†ç»“æœï¼Œç§»é™¤ä¸´æ—¶å­—æ®µ
        cleaned_result = clean_result_for_csv(result)

        logging.info(f"âœ… Sample {sample_id} processed successfully")
        return cleaned_result, None
            
        # except Exception as e:
        #     error_msg = str(e)
        #     logging.error(f"âŒ Sample {sample_id} attempt {attempt + 1} failed: {error_msg}")
        #
        #     if attempt < max_retries:
        #         # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
        #         wait_time = (attempt + 1) * 2  # 2, 4 seconds
        #         logging.info(f"Waiting {wait_time} seconds before retry...")
        #         time.sleep(wait_time)
        #     else:
        #         logging.error(f"âŒ Sample {sample_id} failed after {max_retries + 1} attempts")
        #         return None, error_msg

def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='é€šç”¨å› æœæ¨ç†å®éªŒè„šæœ¬')
    parser.add_argument('--input-file', type=str, required=True,
                       help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num-experiments', type=int, default=110,
                       help='å®éªŒæ ·æœ¬æ•°é‡ (é»˜è®¤: 110)')
    parser.add_argument('--backend', type=str, default='qwen-72b',
                       choices=['qwen-72b', 'qwen-7b','llama-8b','gpt-3.5','claude-3.5','gemini-1.5-flash','ds-r1','claude-3.7-sonnet','o3-mini','gpt-5'],
                       help='LLMåç«¯ (é»˜è®¤: deepinfra)')
    parser.add_argument('--max-retries', type=int, default=1,
                       help='æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 1)')
    parser.add_argument('--search-max-results', type=int, default=3,
                       help='æ¯ä¸ªæœç´¢æŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•° (é»˜è®¤: 3)')
    parser.add_argument('--mode', type=int, default=0,
                       help='0:causal reasoning (é»˜è®¤) 1:cot 2:ws')
    
    args = parser.parse_args()
    
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    # ä»æ–‡ä»¶è·¯å¾„æå–æ•°æ®é›†åç§°ï¼Œå»æ‰.csvåç¼€
    from pathlib import Path
    dataset_name = Path(args.input_file).stem



    print(f"ğŸ“‹ é…ç½®:")
    print(f"  æ•°æ®æ–‡ä»¶: {args.input_file}")
    print(f"  å®éªŒæ•°é‡: {args.num_experiments}")
    print(f"  åç«¯: {args.backend}")
    print(f"  æ¨¡å¼: {args.mode}")
    
    # åŠ è½½æ•°æ®é›†
    try:
        df = pd.read_csv(args.input_file)
    except Exception as e:
        logging.error(f"âŒ Failed to load dataset: {e}")
        print(f"âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {args.input_file}")
        return
    
    # å‡†å¤‡æ ·æœ¬
    num_experiments = min(args.num_experiments, len(df))
    sampled_df = df.sample(n=num_experiments, replace=False, random_state=42)

    input_samples = []
    for idx, row in sampled_df.iterrows():
        premise, hypothesis = safe_extract_premise_hypothesis(row["input"], idx)
        
        # ğŸ†• åˆ›å»ºå®Œæ•´çš„æ ·æœ¬å­—å…¸ï¼Œé¢„å…ˆåˆå§‹åŒ–æ‰€æœ‰å¯èƒ½éœ€è¦çš„å­—æ®µ
        sample = {
            # åŸºç¡€ä¿¡æ¯
            "sample_id": idx,
            "sample_input": row["input"],
            "sample_label": row["label"],
            "sample_num_variables": row.get("num_variables", 2),
            "sample_template": row.get("template", "unknown"),
            "premise": premise,
            "hypothesis": hypothesis,
            
            # ğŸ†• å›¾ç»“æ„å­—æ®µ - é¢„å…ˆåˆå§‹åŒ–ä¸ºç©º
            "nodes": [],
            "undirected_edges": [],
            "directed_edges": [],
            "v_structures": [],
            
            # ğŸ†• å¤„ç†è¿‡ç¨‹å­—æ®µ - é¢„å…ˆåˆå§‹åŒ–
            "_broad_search_summary": ""
            }
        input_samples.append(sample)

    logging.info(f"âœ… Prepared {len(input_samples)} samples with pre-initialized fields")
    
    # è¿æ¥LLM
    try:
        if args.backend == "qwen-72b":
            model_id = "Qwen/Qwen2.5-72B-Instruct"
            base_url = "https://api.deepinfra.com/v1/openai"
            api_key_env = "DEEPINFRA_API_KEY"
        elif args.backend == "llama-8b":
            model_id = "meta-llama/Meta-Llama-3-8B-Instruct"  # â† æ”¹ä¸ºå®é™…å¯ç”¨çš„æ¨¡å‹å
            base_url = "https://api.deepinfra.com/v1/openai"
            api_key_env = "DEEPINFRA_API_KEY"
        elif args.backend == "qwen-7b":
            model_id = "Qwen/Qwen2.5-7B-Instruct"
            base_url = "https://api.deepinfra.com/v1/openai"
            api_key_env = "DEEPINFRA_API_KEY"
        elif args.backend == "gpt-3.5":
            model_id = "gpt-3.5-turbo"
            base_url = "https://4.0.wokaai.com/v1/"
            api_key_env = "WOKKA_API_KEY"
        elif args.backend == "gpt-5":
            model_id = "gpt-5"
            base_url = "https://4.0.wokaai.com/v1/"
            api_key_env = "WOKKA_API_KEY"
        elif args.backend == "claude-3.5":
            model_id = "claude-3-5"
            base_url = "https://api.deepinfra.com/v1/openai"
            api_key_env = "DEEPINFRA_API_KEY"
        elif args.backend == "claude-3.7-sonnet":
            model_id = "anthropic/claude-3-7-sonnet-latest"
            base_url = "https://api.deepinfra.com/v1/openai"
            api_key_env = "DEEPINFRA_API_KEY"
        elif args.backend == "gemini-1.5-flash":
            model_id = "gemini-1.5-flash"
            base_url = "https://4.0.wokaai.com/v1/"
            api_key_env = "WOKKA_API_KEY"
        elif args.backend == "ds-r1":
            model_id = "deepseek-ai/DeepSeek-R1"
            base_url = "https://api.deepinfra.com/v1/openai"
            api_key_env = "DEEPINFRA_API_KEY"
        elif args.backend == "o3-mini":
            model_id = "o3-mini"
            base_url = "https://4.0.wokaai.com/v1/"
            api_key_env = "WOKKA_API_KEY"
            
        client = OpenAIClient(
            model_id=model_id,
            concurrency=1,
            base_url=base_url,
            api_key_env=api_key_env
        )
        
        logging.info(f"âœ… Created {args.backend} client")
    except Exception as e:
        logging.error(f"âŒ Failed to create client: {e}")
        return

    # try:
    search_client = None
    if args.mode:
        search_client = DuckDuckGoSearchClient(
            max_results=args.search_max_results
        )
        logging.info("DuckDuckGoå·²å¯ç”¨")

    initial_construct_stage = InitialConstructStage(client=client,prompt_type=dataset_name.split('_')[0], search_client=search_client)
    LLM_DAG_complement_stage = LLMDAGComplementStage(client=client, search_client=search_client)
    broad_retrieval_stage = BroadRetrievalStage(client=client, search_client=search_client)
    skeleton_stage = UndirectedSkeletonStage(client=client, search_client=search_client)
    v_structures_stage = VStructuresStage(client=client, search_client=search_client)
    meek_rules_stage = MeekRulesStage(client=client, search_client=search_client)
    hypothesis_evaluation_stage = HypothesisEvaluationStage(client=client, search_client=search_client)
    kg_client = CNKnowledgeGraphClient()
    knowledge_graph_stage = KnowledgeGraphRetrievalStage(client=client, kg_client=kg_client)
    rag_client = RAGClient(
        max_search_results=args.search_max_results
    )
    rag_enhancement_stage = RAGEnhancementStage(client=client, rag_client=rag_client)

    if args.mode == 0:
        print("mode0: causal reasoning")
        logging.info("mode0: causal reasoning")
        stages = [skeleton_stage, v_structures_stage, meek_rules_stage, hypothesis_evaluation_stage]

    elif args.mode == 1:
        print("mode1: causal-cot")
        logging.info("mode1: causal-cot")
        stages = [initial_construct_stage, LLM_DAG_complement_stage, skeleton_stage, v_structures_stage, meek_rules_stage, hypothesis_evaluation_stage]

    elif args.mode == 2:
        print("mode2: ws")
        logging.info("mode2: ws")
        stages = [initial_construct_stage, broad_retrieval_stage, skeleton_stage, v_structures_stage, meek_rules_stage, hypothesis_evaluation_stage]

    elif args.mode == 3:  # çŸ¥è¯†å›¾è°±æ¨¡å¼
        print("mode3: kg_search")
        logging.info("mode3: kg_search")
        stages = [
            initial_construct_stage,
            knowledge_graph_stage,  # ä½¿ç”¨çŸ¥è¯†å›¾è°±æ£€ç´¢
            skeleton_stage,
            v_structures_stage,
            meek_rules_stage,
            hypothesis_evaluation_stage
        ]
    if args.mode == 4:
        print("mode4: rag_enhanced")
        logging.info("mode4: rag_enhanced")
        stages = [
            initial_construct_stage,
            rag_enhancement_stage,  # RAGå¢å¼ºé˜¶æ®µ
            skeleton_stage,
            v_structures_stage,
            meek_rules_stage,
            hypothesis_evaluation_stage
        ]

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    job_id = f"{dataset_name}_{args.backend}_mode{args.mode}_{timestamp}"

    logger = ExperimentLogger(LOGS_DIR, job_id)

    pipeline = CausalDiscoveryPipeline(
        stages=stages,
        logger=logger,
    )

    # except Exception as e:
    #     logging.error(f"âŒ ç®¡é“åˆ›å»ºå¤±è´¥: {e}")
    #     return
    
    # å¤„ç†æ ·æœ¬
    results = []
    failed_ids = []
    failed_details = []
    
    start_time = time.time()
    
    print(f"\nğŸ”„ å¼€å§‹å¤„ç† {len(input_samples)} ä¸ªæ ·æœ¬...")
    
    for i, sample in enumerate(tqdm(input_samples, desc="Processing samples")):
        print(f"\n--- Sample {i+1}/{len(input_samples)} (ID: {sample['sample_id']}) ---")
        
        result, error = safe_process_sample(pipeline, sample, max_retries=args.max_retries)
        
        if result is not None:
            results.append(result)
            print(f"âœ… Success")
        else:
            failed_ids.append(sample["sample_id"])
            failed_details.append({"sample_id": sample["sample_id"], "error": error})
            print(f"âŒ Failed: {error}")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # ç»“æœç»Ÿè®¡
    success_count = len(results)
    failure_count = len(failed_ids)
    success_rate = (success_count / len(input_samples)) * 100 if input_samples else 0
    
    print(f"\nğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(input_samples)}")
    print(f"  æˆåŠŸå¤„ç†: {success_count}")
    print(f"  å¤„ç†å¤±è´¥: {failure_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"  æ€»è€—æ—¶: {total_time:.1f} ç§’")
    print(f"  å¹³å‡æ¯ä¸ªæ ·æœ¬: {total_time/len(input_samples):.1f} ç§’")
    
    if failed_ids:
        print(f"\nâŒ å¤±è´¥çš„æ ·æœ¬ IDs: {failed_ids}")
        for detail in failed_details:
            print(f"  Sample {detail['sample_id']}: {detail['error']}")
    
    # åå¤„ç†ç»“æœ
    if success_count > 0:
        try:
            df_results = pd.read_csv(str(logger.log_file))
            
            # è½¬æ¢æ ‡ç­¾
            def convert_label(label):
                if isinstance(label, str):
                    return 1 if label.lower() in ['yes', '1', 'true'] else 0
                return int(label)
            
            df_results["sample_label_binary"] = df_results["sample_label"].apply(convert_label)
            df_results["hypothesis_label_binary"] = df_results["hypothesis_label"].astype(int)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            tp = ((df_results["hypothesis_label_binary"] == 1) & (df_results["sample_label_binary"] == 1)).sum()
            tn = ((df_results["hypothesis_label_binary"] == 0) & (df_results["sample_label_binary"] == 0)).sum()
            fp = ((df_results["hypothesis_label_binary"] == 1) & (df_results["sample_label_binary"] == 0)).sum()
            fn = ((df_results["hypothesis_label_binary"] == 0) & (df_results["sample_label_binary"] == 1)).sum()
            
            total = len(df_results)
            accuracy = (tp + tn) / total if total > 0 else 0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ (åŸºäº{success_count}ä¸ªæˆåŠŸæ ·æœ¬):")
            print(f"  True Positives: {tp}")
            print(f"  True Negatives: {tn}")
            print(f"  False Positives: {fp}")
            print(f"  False Negatives: {fn}")
            print(f"  Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall:    {recall:.4f}")
            print(f"  F1 Score:  {f1:.4f}")
            
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {logger.log_file}")
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = logger.log_file.parent / f"{logger.log_file.stem}_statistics.txt"
            with open(stats_file, 'w', encoding='utf-8') as f:
                f.write(f"{dataset_name}å®éªŒç»Ÿè®¡æŠ¥å‘Š\n")
                f.write("=" * 60 + "\n\n")
                
                f.write("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:\n")
                f.write(f"  æ•°æ®é›†: {dataset_name}\n")
                f.write(f"  æ•°æ®æ–‡ä»¶: {args.input_file}\n")
                f.write(f"  æ€»æ ·æœ¬æ•°: {len(input_samples)}\n")
                f.write(f"  æˆåŠŸå¤„ç†: {success_count}\n")
                f.write(f"  å¤„ç†å¤±è´¥: {failure_count}\n")
                f.write(f"  æˆåŠŸç‡: {success_rate:.1f}%\n")
                f.write(f"  åç«¯: {args.backend}\n")
                f.write(f"  æ¨¡å¼: {args.mode}\n")  # ğŸ†•
                f.write(f"  æ€»è€—æ—¶: {total_time:.1f} ç§’\n")
                f.write(f"  å¹³å‡æ¯ä¸ªæ ·æœ¬: {total_time/len(input_samples):.1f} ç§’\n\n")
                
                if failed_ids:
                    f.write("âŒ å¤±è´¥çš„æ ·æœ¬:\n")
                    for detail in failed_details:
                        f.write(f"  Sample {detail['sample_id']}: {detail['error']}\n")
                    f.write("\n")
                
                f.write(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ (åŸºäº{success_count}ä¸ªæˆåŠŸæ ·æœ¬):\n")
                f.write(f"  True Positives: {tp}\n")
                f.write(f"  True Negatives: {tn}\n")
                f.write(f"  False Positives: {fp}\n")
                f.write(f"  False Negatives: {fn}\n")
                f.write(f"  Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)\n")
                f.write(f"  Precision: {precision:.4f}\n")
                f.write(f"  Recall:    {recall:.4f}\n")
                f.write(f"  F1 Score:  {f1:.4f}\n\n")
                
                f.write(f"ğŸ“„ è¯¦ç»†å®éªŒæ•°æ®: {logger.log_file.name}\n")
            
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: {stats_file}")
            
        except Exception as e:
            logging.error(f"âŒ åå¤„ç†å¤±è´¥: {e}")
    
    print(f"\nğŸ‰ {dataset_name}å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {logger.log_file}")

if __name__ == "__main__":
    main()
