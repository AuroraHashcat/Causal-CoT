import logging
from abc import ABC, abstractmethod
from typing import Any, Optional, List
import json
import re
import os
from datetime import datetime

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from llm_client import BaseLLMClient
from utils import load_prompts, extract_causal_skeleton_json, extract_v_structures_json, \
    extract_directed_edges_literal_format_json, extract_hypothesis_answer, extract_undirected_edges_literal_format_json, \
    extract_initial_construct_json
from search_client import DuckDuckGoSearchClient


class Stage(ABC):
    """
    Base class for all stages in the pipeline.
    Each subclass needs to implement the `prompt_template` attribute.
    """
    prompts: dict[str, str] = load_prompts()
    prompt_template: str = None
    
    # ğŸ†• ç±»çº§åˆ«çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«
    _shared_log_file = None
    _log_initialized = False

    def __init__(self, client: BaseLLMClient, search_client=None):
        self.client = client
        self.search_client = search_client
        if self.prompt_template is None:
            raise ValueError("Subclasses must define a prompt_template.")
        
        # ğŸ†• åˆå§‹åŒ–å…±äº«æ—¥å¿—æ–‡ä»¶ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰- ä¿®å¤è°ƒç”¨æ–¹å¼
        if not Stage._log_initialized:
            Stage._initialize_shared_log()  # ä½¿ç”¨ç±»åè°ƒç”¨ï¼Œä¸æ˜¯self

    @classmethod
    def _initialize_shared_log(cls):
        """åˆå§‹åŒ–å…±äº«çš„æ—¥å¿—æ–‡ä»¶"""
        try:
            # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„å’Œæ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            current_file = Path(__file__).resolve()
            project_root = current_file.parent.parent.parent
            logs_dir = project_root / "logs"
            
            print(f"ğŸ”§ æ—¥å¿—ç›®å½•è·¯å¾„: {logs_dir}")
            
            # åˆ›å»ºæ—¥å¿—ç›®å½•
            logs_dir.mkdir(parents=True, exist_ok=True)
            print(f"âœ… æ—¥å¿—ç›®å½•åˆ›å»ºæˆåŠŸ: {logs_dir}")
            
            # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            cls._shared_log_file = logs_dir / f"llm_responses_{timestamp}.log"
            
            print(f"ğŸ“ å‡†å¤‡åˆ›å»ºæ—¥å¿—æ–‡ä»¶: {cls._shared_log_file}")
            
            # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¹¶å†™å…¥å¤´éƒ¨ä¿¡æ¯
            with open(cls._shared_log_file, 'w', encoding='utf-8') as f:
                f.write(f"LLM Response Log - Started at {timestamp}\n")
                f.write("="*80 + "\n\n")
                f.flush()
                os.fsync(f.fileno())
            
            cls._log_initialized = True
            print(f"âœ… å…±äº«LLMæ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {cls._shared_log_file}")
            
            # ğŸ†• éªŒè¯æ–‡ä»¶æ˜¯å¦ç¡®å®å­˜åœ¨
            if cls._shared_log_file.exists():
                print(f"âœ… æ—¥å¿—æ–‡ä»¶éªŒè¯æˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {cls._shared_log_file.stat().st_size} å­—èŠ‚")
            else:
                print(f"âŒ æ—¥å¿—æ–‡ä»¶åˆ›å»ºåä¸å­˜åœ¨!")
                cls._shared_log_file = None
            
        except Exception as e:
            print(f"âŒ æ—¥å¿—æ–‡ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            print(f"å®Œæ•´é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            cls._shared_log_file = None
            cls._log_initialized = True

    def _log_llm_response(self, stage_name: str, prompt: str, response: str, usage=None, operation: str = "main"):
        """ç»Ÿä¸€çš„LLMå“åº”æ—¥å¿—è®°å½•æ–¹æ³• - å®æ—¶å†™å…¥ç‰ˆæœ¬"""
        timestamp = datetime.now().isoformat()
        
        # ğŸ†• æ£€æŸ¥å…±äº«æ—¥å¿—æ–‡ä»¶æ˜¯å¦å¯ç”¨
        if Stage._shared_log_file is None:
            print(f"âš ï¸ æ—¥å¿—æ–‡ä»¶ä¸å¯ç”¨ï¼Œè·³è¿‡è®°å½•: {stage_name} - {operation}")
            # ğŸ†• ä¸´æ—¶åˆ›å»ºå•ç‹¬çš„æ—¥å¿—æ–‡ä»¶ä½œä¸ºåº”æ€¥æ–¹æ¡ˆ
            try:
                temp_log_file = Path(__file__).parent / f"temp_llm_log_{stage_name}_{operation}_{datetime.now().strftime('%H%M%S')}.txt"
                with open(temp_log_file, 'w', encoding='utf-8') as f:
                    f.write(f"=== {stage_name} - {operation} ===\n")
                    f.write(f"Timestamp: {timestamp}\n\n")
                    f.write("PROMPT:\n")
                    f.write(prompt)
                    f.write("\n\nRESPONSE:\n")
                    f.write(response)
                print(f"ğŸ“ åº”æ€¥æ—¥å¿—å·²ä¿å­˜: {temp_log_file}")
            except Exception as e:
                print(f"âŒ è¿åº”æ€¥æ—¥å¿—ä¹Ÿå¤±è´¥äº†: {e}")
            return
        
        # ğŸ†• å®æ—¶å†™å…¥åˆ°å…±äº«æ—¥å¿—æ–‡ä»¶
        try:
            with open(Stage._shared_log_file, 'a', encoding='utf-8') as f:
                # å†™å…¥åˆ†éš”çº¿å’Œæ ‡é¢˜
                f.write(f"\n{'='*80}\n")
                f.write(f"[{timestamp}] {stage_name} - {operation.upper()}\n")
                f.write(f"{'='*80}\n")
                
                # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
                f.write(f"Text Lengths: Prompt={len(prompt)} chars, Response={len(response)} chars\n\n")
                
                # Promptå†…å®¹
                f.write("PROMPT:\n")
                f.write("-" * 40 + "\n")
                f.write(prompt)
                f.write("\n" + "-" * 40 + "\n\n")
                
                # Responseå†…å®¹
                f.write("RESPONSE:\n")
                f.write("-" * 40 + "\n")
                f.write(response)
                f.write("\n" + "-" * 40 + "\n\n")
                
                # ğŸ†• ç«‹å³åˆ·æ–°åˆ°ç£ç›˜ï¼Œç¡®ä¿å®æ—¶å¯è§
                f.flush()
                os.fsync(f.fileno())
            
            # æ§åˆ¶å°è¾“å‡ºç¡®è®¤
            print(f"ğŸ“ å·²è®°å½•: {stage_name} - {operation} ({len(response)} chars)")
                
        except Exception as e:
            print(f"âŒ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
            logging.error(f"Failed to write LLM log to file: {e}")
        
        # æ§åˆ¶å°æ—¥å¿—è®°å½•ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
        logging.info(f"[{stage_name}] LLM {operation} call completed")
        logging.info(f"[{stage_name}] Text lengths - Prompt: {len(prompt)} chars, Response: {len(response)} chars")

    def _execute_searches(self, queries: List[str]) -> dict:
        """æ‰§è¡Œæœç´¢æŸ¥è¯¢"""
        search_results = {}
        
        if not queries:
            return search_results
        
        try:
            # ä½¿ç”¨DuckDuckGoæœç´¢å®¢æˆ·ç«¯
            search_client = DuckDuckGoSearchClient(max_results=3)
            
            for query in queries:
                try:
                    logging.info(f"Executing DuckDuckGo search: {query[:50]}...")
                    results = search_client.search(query)
                    if results:
                        search_results[query] = results
                        logging.info(f"âœ… Found {len(results)} results for: {query[:30]}...")
                    else:
                        logging.warning(f"âš ï¸  No results for: {query[:30]}...")
                        search_results[query] = []
                    
                except Exception as e:
                    logging.error(f"Search failed for query '{query}': {e}")
                    search_results[query] = []
                    
        except Exception as e:
            logging.error(f"Failed to initialize DuckDuckGo search client: {e}")
        
        return search_results
    
    def _parse_query_list(self, response: str) -> List[str]:
        """è§£ææŸ¥è¯¢åˆ—è¡¨ - ç»Ÿä¸€æ–¹æ³•"""
        try:
            return json.loads(response.strip())
        except json.JSONDecodeError:
            # å›é€€è§£ææ–¹æ³•
            match = re.search(r'\[(.*?)\]', response, re.DOTALL)
            if match:
                try:
                    return json.loads('[' + match.group(1) + ']')
                except json.JSONDecodeError:
                    pass
            
            # æœ€åçš„å›é€€æ–¹æ¡ˆ
            lines = [line.strip().strip('"').strip("'") 
                    for line in response.split('\n') if line.strip()]
            return [line for line in lines 
                   if line and not line.startswith('[') and not line.startswith(']')][:4]


class InitialConstructStage(Stage):
    """
    Stage -1: Initial graph construction stage - åˆ›å»ºåˆå§‹çš„å›¾ç»“æ„
    åœ¨æ‰€æœ‰å…¶ä»–é˜¶æ®µä¹‹å‰è¿è¡Œï¼ŒåŸºäºpremiseå’Œhypothesiså»ºç«‹åŸºç¡€å›¾ç»“æ„
    """
    def __init__(self, client, search_client=None, prompt_type="causal"):
        # æ ¹æ®ç±»å‹é€‰æ‹©prompt
        self.prompt_template = Stage.prompts[f"initial_construct_{prompt_type}"]
        super().__init__(client, search_client)
    
    def process(self, input_data: dict[str, Any]) -> dict[str, Any]:
        # 1. éªŒè¯è¾“å…¥
        if "premise" not in input_data or "hypothesis" not in input_data:
            raise ValueError("InitialConstructStage: Input data must contain premise and hypothesis.")
        
        # 2. æ„å»ºprompt
        prompt = self.prompt_template.format(
            premise=input_data["premise"],
            hypothesis=input_data["hypothesis"]
        )

        # åœ¨æ¯ä¸ªstageçš„promptæ„å»ºåè¿½åŠ 
        causal_question = input_data.get("causal_question", "")
        if causal_question:
            prompt += f"\n\nPay particular attention to whether the causal question holds trueï¼š {causal_question}\n"
        
        # 3. å‘é€ç»™LLMè¿›è¡Œåˆå§‹æ„å›¾
        logging.info("InitialConstructStage: Creating initial graph structure")
        response, usage = self.client.complete(prompt=prompt)
        
        # æ·»åŠ LLMå“åº”æ—¥å¿—
        self._log_llm_response("InitialConstructStage", prompt, response, usage, "main")
        
        # 4. ğŸ†• ç´¯ç§¯å¼æ·»åŠ åˆ°å·²æœ‰ç»“æ„
        try:
            structure = extract_initial_construct_json(answer=response)
            
            # ğŸ†• ç´¯ç§¯æ·»åŠ nodes
            if "nodes" in structure and structure["nodes"]:
                input_data["nodes"].extend(structure["nodes"])
                logging.info(f"InitialConstructStage: Added {len(structure['nodes'])} nodes, total: {len(input_data['nodes'])}")
            
            # ğŸ†• ç´¯ç§¯æ·»åŠ directed_edges
            if "directed_edges" in structure and structure["directed_edges"]:
                input_data["directed_edges"].extend(structure["directed_edges"])
                logging.info(f"InitialConstructStage: Added {len(structure['directed_edges'])} directed edges, total: {len(input_data['directed_edges'])}")
            
            # ğŸ†• ç´¯ç§¯æ·»åŠ undirected_edges
            if "undirected_edges" in structure and structure["undirected_edges"]:
                input_data["undirected_edges"].extend(structure["undirected_edges"])
                logging.info(f"InitialConstructStage: Added {len(structure['undirected_edges'])} undirected edges, total: {len(input_data['undirected_edges'])}")
            
            # è®¾ç½®å…¶ä»–å­—æ®µ
            if "causal_question" in structure:
                input_data["causal_question"] = structure["causal_question"]
        
            logging.info(f"InitialConstructStage: Successfully built initial structure")
            
        except Exception as e:
            logging.error(f"InitialConstructStage: Error extracting structure: {e}")
            logging.debug(f"InitialConstructStage: Problematic response: {response}")
        return input_data

class LLMDAGComplementStage(Stage):
    """
    Stage for LLM to complement and refine the causal DAG.
    """
    prompt_template = Stage.prompts["LLM_DAG_complement"]

    def process(self, input_data: dict[str, Any]) -> dict[str, Any]:
        # éªŒè¯è¾“å…¥
        required_keys = {"premise", "hypothesis","nodes", "directed_edges", "undirected_edges"}
        if not required_keys.issubset(input_data):
            raise ValueError(f"LLM_DAG_ComplementStage: Input data must contain: {', '.join(required_keys)}.")

        # æ„å»ºprompt
        prompt = self.prompt_template.format(
            premise=input_data["premise"],
            hypothesis = input_data["hypothesis"],
            nodes=input_data["nodes"],
            directed_edges=input_data["directed_edges"],
            undirected_edges=input_data["undirected_edges"]
        )

        causal_question = input_data.get("causal_question", "")
        if causal_question:
            prompt += f"\n\nPay particular attention to whether the causal question holds trueï¼š {causal_question}\n"

        logging.info("LLM_DAG_ComplementStage: Sending prompt to LLM.")
        response, usage = self.client.complete(prompt=prompt)

        # æ·»åŠ LLMå“åº”æ—¥å¿—
        self._log_llm_response("LLM_DAG_ComplementStage", prompt, response, usage, "main")

        # è§£æLLMè¡¥å……åçš„ç»“æ„
        try:
            structure = extract_initial_construct_json(answer=response)
            # ç´¯ç§¯æ·»åŠ nodes
            if "nodes" in structure and structure["nodes"]:
                input_data["nodes"].extend(structure["nodes"])
                logging.info(f"LLM_DAG_ComplementStage: Added {len(structure['nodes'])} nodes, total: {len(input_data['nodes'])}")
            # ç´¯ç§¯æ·»åŠ directed_edges
            if "directed_edges" in structure and structure["directed_edges"]:
                input_data["directed_edges"].extend(structure["directed_edges"])
                logging.info(f"LLM_DAG_ComplementStage: Added {len(structure['directed_edges'])} directed edges, total: {len(input_data['directed_edges'])}")
            # ç´¯ç§¯æ·»åŠ undirected_edges
            if "undirected_edges" in structure and structure["undirected_edges"]:
                input_data["undirected_edges"].extend(structure["undirected_edges"])
                logging.info(f"LLM_DAG_ComplementStage: Added {len(structure['undirected_edges'])} undirected edges, total: {len(input_data['undirected_edges'])}")
            logging.info("LLM_DAG_ComplementStage: Successfully complemented DAG structure")
        except Exception as e:
            logging.error(f"LLM_DAG_ComplementStage: Error extracting complemented structure: {e}")
            logging.debug(f"LLM_DAG_ComplementStage: Problematic response: {response}")
        return input_data

class BroadRetrievalStage(Stage):
    """
    Stage 0: Perform broad retrieval for general background and context,
    then enhance the initial graph structure with domain knowledge
    """
    prompt_template = Stage.prompts["web_search"]
    enhance_prompt_template = Stage.prompts["search_results_enhancement"]
    
    def process(self, input_data: dict[str, Any]) -> dict[str, Any]:
        # 1. éªŒè¯è¾“å…¥
        required_keys = {"premise", "nodes"}
        if not required_keys.issubset(input_data):
            raise ValueError(f"BroadRetrievalStage: Input data must contain: {', '.join(required_keys)}.")

        # 2. ç”Ÿæˆå¹¿æ³›çš„èƒŒæ™¯æœç´¢æŸ¥è¯¢
        broad_queries = self._generate_broad_queries(input_data)
        
        # 3. æ‰§è¡Œæœç´¢
        search_results = self._execute_searches(broad_queries)
        
        # ğŸ†• 4. å¦‚æœæœ‰æœç´¢ç»“æœï¼Œå¢å¼ºå›¾ç»“æ„ï¼›å¦‚æœæ²¡æœ‰ï¼Œä¿æŒåŸå§‹ç»“æ„
        if search_results and any(search_results.values()):
            logging.info("BroadRetrievalStage: Search results available, enhancing graph structure")
            input_data = self._enhance_graph_with_search_results(input_data, search_results)
        else:
            logging.info("BroadRetrievalStage: No search results, keeping original graph structure")
        
        logging.info(f"BroadRetrievalStage: Completed with {len(broad_queries)} queries")
        return input_data

    def _generate_broad_queries(self, input_data: dict[str, Any]) -> List[str]:
        """ç”Ÿæˆç²¾å‡†çš„åˆå§‹æœç´¢æŸ¥è¯¢"""
        current_nodes = input_data.get("nodes", [])
        current_undirected_edges = input_data.get("undirected_edges", [])
        current_directed_edges = input_data.get("directed_edges", [])
        
        # ä½¿ç”¨ self.prompt_template æ„å»º prompt
        search_prompt = self.prompt_template.format(
            premise=input_data.get('premise', ''),
            hypothesis=input_data.get('hypothesis', ''),
            nodes=current_nodes,
            directed_edges=current_directed_edges,
            undirected_edges=current_undirected_edges
        )

        causal_question = input_data.get("causal_question", "")
        if causal_question:
            search_prompt += f"\n\nPay particular attention to whether the causal question holds trueï¼š {causal_question}\n"

        try:
            response, usage = self.client.complete(prompt=search_prompt)
            
            # æ·»åŠ æŸ¥è¯¢ç”Ÿæˆæ—¥å¿—
            self._log_llm_response("BroadRetrievalStage", search_prompt, response, usage, "query_generation")
            
            queries = self._parse_query_list(response)
            
            # é™åˆ¶ä¸ºæœ€å¤š3ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢ä¸è¶…è¿‡4ä¸ªè¯
            limited_queries = []
            for query in queries[:3]:  # æœ€å¤š3ä¸ª
                words = query.split()
                if len(words) <= 4:  # æ¯ä¸ªæŸ¥è¯¢æœ€å¤š4ä¸ªè¯
                    limited_queries.append(query)
                else:
                    limited_queries.append(' '.join(words[:4]))  # æˆªæ–­åˆ°4ä¸ªè¯
        
            logging.info(f"BroadRetrievalStage: Generated {len(limited_queries)} queries: {limited_queries}")
            return limited_queries
        
        except Exception as e:
            logging.error(f"Failed to generate broad queries: {e}")
            return []  # å¦‚æœå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯å›é€€æŸ¥è¯¢

    def _enhance_graph_with_search_results(self, input_data: dict[str, Any], search_results: dict) -> dict[str, Any]:
        """ğŸ†• ä½¿ç”¨æœç´¢ç»“æœç´¯ç§¯å¢å¼ºå›¾ç»“æ„"""
        try:
            # è·å–å½“å‰å›¾ç»“æ„ä¿¡æ¯ç”¨äºprompt
            current_nodes = input_data.get("nodes", [])
            current_undirected_edges = input_data.get("undirected_edges", [])
            current_directed_edges = input_data.get("directed_edges", [])
            
            # ä½¿ç”¨ self.prompt_template æ„å»º prompt
            enhancement_prompt = self.enhance_prompt_template.format(
                premise=input_data.get('premise', ''),
                hypothesis=input_data.get('hypothesis', ''),
                nodes=current_nodes,
                directed_edges=current_directed_edges,
                undirected_edges=current_undirected_edges
            )

            causal_question = input_data.get("causal_question", "")
            if causal_question:
                enhancement_prompt += f"\n\nPay particular attention to whether the causal question holds trueï¼š {causal_question}\n"
            
            # å‘é€ç»™LLMè¿›è¡Œå›¾å¢å¼º
            response, usage = self.client.complete(prompt=enhancement_prompt)
            
            # è®°å½•å›¾å¢å¼ºè¿‡ç¨‹
            self._log_llm_response("BroadRetrievalStage", enhancement_prompt, response, usage, "graph_enhancement")
            
            # è§£æå¢å¼ºç»“æœ
            enhanced_structure = self._parse_graph_enhancement_response(response)
            
            # ğŸ†• ç´¯ç§¯å¼æ·»åŠ æ–°çš„å›¾å…ƒç´ 
            if enhanced_structure:
                added_nodes = 0
                added_directed = 0
                added_undirected = 0
                
                # ğŸ†• æ·»åŠ æ–°èŠ‚ç‚¹ï¼ˆé¿å…é‡å¤ï¼‰
                if "nodes" in enhanced_structure and enhanced_structure["nodes"]:
                    existing_node_ids = {node.get("id") for node in input_data["nodes"]}
                    new_nodes = [node for node in enhanced_structure["nodes"] 
                               if node.get("id") not in existing_node_ids]
                    if new_nodes:
                        input_data["nodes"].extend(new_nodes)
                        added_nodes = len(new_nodes)
                
                # ğŸ†• æ·»åŠ æ–°çš„æœ‰å‘è¾¹ï¼ˆé¿å…é‡å¤ï¼‰
                if "directed_edges" in enhanced_structure and enhanced_structure["directed_edges"]:
                    existing_directed = {(edge["from"], edge["to"]) for edge in input_data["directed_edges"]}
                    new_directed = [edge for edge in enhanced_structure["directed_edges"]
                                    if (edge["from"], edge["to"]) not in existing_directed]
                    if new_directed:
                        input_data["directed_edges"].extend(new_directed)
                        added_directed = len(new_directed)
                
                # ğŸ†• æ·»åŠ æ–°çš„æ— å‘è¾¹ï¼ˆé¿å…é‡å¤ï¼‰
                if "undirected_edges" in enhanced_structure and enhanced_structure["undirected_edges"]:
                    existing_undirected = {tuple(sorted(edge)) for edge in input_data["undirected_edges"]}
                    new_undirected = [edge for edge in enhanced_structure["undirected_edges"]
                                      if tuple(sorted(edge)) not in existing_undirected]
                    if new_undirected:
                        input_data["undirected_edges"].extend(new_undirected)
                        added_undirected = len(new_undirected)
                
                logging.info(f"BroadRetrievalStage: Enhanced graph - Added {added_nodes} nodes, {added_directed} directed edges, {added_undirected} undirected edges")
                logging.info(f"BroadRetrievalStage: Total graph size - {len(input_data['nodes'])} nodes, {len(input_data['directed_edges'])} directed edges, {len(input_data['undirected_edges'])} undirected edges")
            else:
                logging.warning("BroadRetrievalStage: No valid enhancement structure extracted")
        
        except Exception as e:
            logging.error(f"BroadRetrievalStage: Graph enhancement failed: {e}")
            # ğŸ†• é”™è¯¯æ—¶ä¸ä¿®æ”¹å·²æœ‰å›¾ç»“æ„
    
        return input_data

    def _parse_graph_enhancement_response(self, response: str) -> dict:
        """è§£æå›¾å¢å¼ºå“åº”"""
        try:
            import json
            import re
            
            # æŸ¥æ‰¾JSONä»£ç å—
            json_match = re.search(r'```(?:json)?\s*({\s*.*?}\s*)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                data = json.loads(json_str)
                
                result = {}
                
                # æå–å„ä¸ªå­—æ®µ
                for key in ["nodes", "undirected_edges", "directed_edges", "enhanced_premise", "domain_insights"]:
                    if key in data:
                        result[key] = data[key]
                
                logging.debug(f"BroadRetrievalStage: Successfully parsed enhancement response with {len(result)} fields")
                return result
            else:
                logging.warning("BroadRetrievalStage: No JSON found in enhancement response")
                return {}
                
        except json.JSONDecodeError as e:
            logging.error(f"BroadRetrievalStage: JSON parsing failed: {e}")
            return {}
        except Exception as e:
            logging.error(f"BroadRetrievalStage: Failed to parse graph enhancement response: {e}")
            return {}

class UndirectedSkeletonStage(Stage):
    """
    Stage 1: Refine the undirected skeleton based on initial construction and search results
    """
    prompt_template = Stage.prompts["undirected_skeleton"]

    def process(self, input_data: dict[str, Any]) -> dict[str, Any]:
        # 1. Validate inputs
        if "premise" not in input_data:
            raise ValueError("Input data must contain Premise.")

        # 3. æ„å»ºå¢å¼ºçš„prompt
        prompt = self._build_enhanced_prompt(input_data)

        causal_question = input_data.get("causal_question", "")
        if causal_question:
            prompt += f"\n\nPay particular attention to whether the causal question holds trueï¼š {causal_question}\n"

        # 4. Send request to LLM
        logging.info("UndirectedSkeletonStage: Sending prompt to LLM.")
        response, usage = self.client.complete(prompt=prompt)

        # ğŸ†• æ·»åŠ LLMå“åº”æ—¥å¿—
        self._log_llm_response("UndirectedSkeletonStage", prompt, response, usage, "main")

        # 5. ğŸ†• ç´¯ç§¯å¼ç²¾ç‚¼å›¾ç»“æ„ - ä¸è¦†ç›–ï¼Œåªæ·»åŠ 
        try:
            skeleton = extract_causal_skeleton_json(answer=response)
            
            # ğŸ†• ç´¯ç§¯æ·»åŠ æ–°èŠ‚ç‚¹ï¼ˆé¿å…é‡å¤ï¼‰
            if "nodes" in skeleton and skeleton["nodes"]:
                existing_node_ids = {node.get("id") for node in input_data["nodes"]}
                new_nodes = [node for node in skeleton["nodes"] 
                           if node.get("id") not in existing_node_ids]
                if new_nodes:
                    input_data["nodes"].extend(new_nodes)
                    logging.info(f"UndirectedSkeletonStage: Added {len(new_nodes)} new nodes, total: {len(input_data['nodes'])}")
            
            # ğŸ†• ç´¯ç§¯æ·»åŠ æ–°æ— å‘è¾¹ï¼ˆé¿å…é‡å¤ï¼‰
            if "undirected_edges" in skeleton and skeleton["undirected_edges"]:
                refined_edges = []
                for edge in skeleton["undirected_edges"]:
                    if isinstance(edge, list) and len(edge) == 2:
                        refined_edges.append(edge)  # ç›´æ¥ç”¨listç»“æ„
                    elif isinstance(edge, dict) and "source" in edge and "target" in edge:
                        refined_edges.append([edge["source"], edge["target"]])
                existing_undirected = {tuple(sorted(e)) for e in input_data["undirected_edges"]}
                new_edges = [e for e in refined_edges if tuple(sorted(e)) not in existing_undirected]
                if new_edges:
                    input_data["undirected_edges"].extend(new_edges)
                    logging.info(f"UndirectedSkeletonStage: Added {len(new_edges)} new undirected edges, total: {len(input_data['undirected_edges'])}")
            
        except Exception as e:
            logging.error("UndirectedSkeletonStage: Error extracting skeleton: %s", e)
            logging.debug("UndirectedSkeletonStage: Problematic response: %s", response)
            # ğŸ†• é”™è¯¯æ—¶ä¸ä¿®æ”¹å·²æœ‰ç»“æ„
            logging.info("UndirectedSkeletonStage: Keeping current graph structure due to extraction failure")

        return input_data

    def _build_enhanced_prompt(self, input_data: dict[str, Any]) -> str:
        """æ„å»ºåŒ…å«æœç´¢ç»“æœçš„å¢å¼ºprompt"""
        base_prompt = self.prompt_template.format(premise=input_data["premise"])
        
        # åˆ é™¤æ‰€æœ‰initial_*ç›¸å…³ä»£ç ï¼Œç°åœ¨ç›´æ¥åŸºäºç°æœ‰çš„nodeså’Œedges
        existing_nodes = input_data.get("nodes", [])
        existing_edges = input_data.get("undirected_edges", [])
        
        if existing_nodes or existing_edges:
            base_prompt += f"\n\nCurrent graph structure to refine:\nNodes: {existing_nodes}\nUndirected_Edges: {existing_edges}"
            base_prompt += "\n\nPlease refine this structure based on careful analysis."
        
        # âœ… ä¿®å¤ï¼šæ·»åŠ å¹¿æ³›æœç´¢ç»“æœ
        if input_data.get('_broad_search_summary'):
            base_prompt += f"\n\nBackground domain context: {input_data['_broad_search_summary']}"
        
        # æ·»åŠ é’ˆå¯¹æ€§æœç´¢ä¸Šä¸‹æ–‡
        if hasattr(self, '_current_search_summary') and self._current_search_summary:
            base_prompt += f"\n\nRelevant domain context: {self._current_search_summary}"
        
        return base_prompt

    def process_batch(self, inputs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        # ç®€åŒ–æ‰¹å¤„ç†å®ç°ï¼Œé€ä¸ªå¤„ç†ä»¥æ”¯æŒåŠ¨æ€æœç´¢
        logging.info("UndirectedSkeletonStage: Processing batch with %d samples.", len(inputs))
        for i, input_data in enumerate(inputs):
            try:
                self.process(input_data)
            except Exception as e:
                logging.error(f"Error processing sample {i}: {e}")
                input_data["nodes"] = None
                input_data["undirected_edges"] = None
        return inputs

class VStructuresStage(Stage):
    """
    Stage for generating the V-structures out of the causal graph and Premise.
    """
    prompt_template = Stage.prompts["v_structures"]

    def process(self, input_data: dict[str, Any]) -> dict[str, Any]:
        # 1. Validate inputs
        required_keys = {"premise", "nodes", "undirected_edges"}
        if not required_keys.issubset(input_data):
            raise ValueError(f"Input data must contain: {', '.join(required_keys)}.")

        # Check for None values from previous stages
        if input_data.get("nodes") is None or input_data.get("undirected_edges") is None:
            logging.warning("VStructuresStage: Previous stage returned None values, skipping processing")
            input_data["v_structures"] = None
            return input_data

        # 3. Build enhanced prompt
        try:
            prompt = self._build_enhanced_prompt(input_data)
            causal_question = input_data.get("causal_question", "")
            if causal_question:
                prompt += f"\n\nPay particular attention to whether the causal question holds trueï¼š {causal_question}\n"
        except Exception as e:
            logging.error("VStructuresStage: Error formatting prompt: %s", e)
            input_data["v_structures"] = None
            return input_data

        # 4. Send request to LLM
        logging.info("VStructuresStage: Sending prompt to LLM.")
        response, usage = self.client.complete(prompt=prompt)

        # ğŸ†• æ·»åŠ LLMå“åº”æ—¥å¿—
        self._log_llm_response("VStructuresStage", prompt, response, usage, "main")
            # ğŸ†• ä¿®æ”¹ä¸ºç´¯ç§¯æ·»åŠ :
        try:
            v_structures_data = extract_v_structures_json(answer=response)
            
            # ğŸ†• ç´¯ç§¯æ·»åŠ vç»“æ„ï¼ˆé¿å…é‡å¤ï¼‰
            if isinstance(v_structures_data, list):
                # å¦‚æœè¿”å›åˆ—è¡¨ï¼Œç›´æ¥æ·»åŠ 
                existing_v_structures = {str(v) for v in input_data.get("v_structures", [])}
                new_v_structures = [v for v in v_structures_data if str(v) not in existing_v_structures]
                input_data["v_structures"].extend(new_v_structures)
                logging.info(f"VStructuresStage: Added {len(new_v_structures)} new v-structures, total: {len(input_data['v_structures'])}")
            elif isinstance(v_structures_data, dict) and "v_structures" in v_structures_data:
                # å¦‚æœè¿”å›å­—å…¸æ ¼å¼
                new_v_structures = v_structures_data["v_structures"]
                if isinstance(new_v_structures, list):
                    existing_v_structures = {str(v) for v in input_data.get("v_structures", [])}
                    unique_new = [v for v in new_v_structures if str(v) not in existing_v_structures]
                    input_data["v_structures"].extend(unique_new)
                    logging.info(f"VStructuresStage: Added {len(unique_new)} new v-structures, total: {len(input_data['v_structures'])}")
        
        except Exception as e:
            logging.error("VStructuresStage: Error extracting V-structures: %s", e)
            logging.debug("VStructuresStage: Problematic response: %s", response)
            # ğŸ†• é”™è¯¯æ—¶ä¸ä¿®æ”¹å·²æœ‰v_structures
            logging.info("VStructuresStage: Keeping current v-structures due to extraction failure")

        return input_data

    def _build_enhanced_prompt(self, input_data: dict[str, Any]) -> str:
        """æ„å»ºåŒ…å«æœç´¢ç»“æœçš„å¢å¼ºprompt"""
        base_prompt = self.prompt_template.format(
            premise=input_data["premise"],
            nodes=input_data["nodes"],
            edges=input_data["undirected_edges"],
        )
        
        # âœ… ä¿®å¤ï¼šæ·»åŠ å¹¿æ³›æœç´¢ç»“æœ
        if input_data.get('_broad_search_summary'):
            base_prompt += f"\n\nDomain background context: {input_data['_broad_search_summary']}"
        
        # æ·»åŠ é’ˆå¯¹æ€§æœç´¢ç»“æœ
        if hasattr(self, '_current_search_summary') and self._current_search_summary:
            base_prompt += f"\n\nAdditional context from focused search:\n{self._current_search_summary}"
        
        return base_prompt

    def process_batch(self, inputs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        logging.info("VStructuresStage: Processing batch with %d samples.", len(inputs))
        for i, input_data in enumerate(inputs):
            try:
                self.process(input_data)
            except Exception as e:
                logging.error(f"Error processing sample {i}: {e}")
                input_data["v_structures"] = None
        return inputs

class MeekRulesStage(Stage):
    """
    Stage for applying Meek's rules to the V-structures.
    """
    prompt_template = Stage.prompts["meek_rules"]

    def process(self, input_data: dict[str, Any]) -> dict[str, Any]:
        # 1. Validate inputs
        required_keys = {"premise", "nodes", "undirected_edges", "v_structures"}
        if not required_keys.issubset(input_data):
            raise ValueError(f"Meek rules stage input data must contain: {', '.join(required_keys)}.")

        # 3. Build enhanced prompt
        prompt = self._build_enhanced_prompt(input_data)

        causal_question = input_data.get("causal_question", "")
        if causal_question:
            prompt += f"\n\nPay particular attention to whether the causal question holds trueï¼š {causal_question}\n"

        # 4. Send request to LLM
        logging.info("MeekRulesStage: Sending prompt to LLM.")
        response, usage = self.client.complete(prompt=prompt)

        # ğŸ†• æ·»åŠ LLMå“åº”æ—¥å¿—
        self._log_llm_response("MeekRulesStage", prompt, response, usage, "main")

        # 5. Unpack responses and update token usage
        try:
            directed_edges = extract_directed_edges_literal_format_json(answer=response)
            undirected_edges = extract_undirected_edges_literal_format_json(answer=response)   
                
            # ğŸ†• Meekè§„åˆ™çš„ç‰¹æ®Šå¤„ç†ï¼šéœ€è¦é‡æ–°å®šå‘è¾¹ï¼Œä½†è¦ä¿æŠ¤å·²æœ‰çš„æœ‰å‘è¾¹
            if directed_edges is not None:
                # ä¿ç•™åŸæœ‰çš„æœ‰å‘è¾¹ï¼Œæ·»åŠ æ–°çš„æœ‰å‘è¾¹
                existing_directed = {(edge.get("from"), edge.get("to")) for edge in input_data.get("directed_edges", [])}
                new_directed = [edge for edge in directed_edges
                                if (edge.get("from"), edge.get("to")) not in existing_directed]
                
                input_data["directed_edges"].extend(new_directed)
                logging.info(f"MeekRulesStage: Added {len(new_directed)} new directed edges, total: {len(input_data['directed_edges'])}")
            
            if undirected_edges is not None:
                # åªæ·»åŠ æ–°çš„æ— å‘è¾¹ï¼Œæ•°æ®ç»“æ„ä¸º ["Node3", "Node4"]
                existing_undirected = {tuple(sorted(edge)) for edge in input_data.get("undirected_edges", [])}
                new_undirected = [edge for edge in undirected_edges
                                if tuple(sorted(edge)) not in existing_undirected]
                input_data["undirected_edges"].extend(new_undirected)
                logging.info(f"MeekRulesStage: Added {len(new_undirected)} new undirected edges, total: {len(input_data['undirected_edges'])}")
            
        except Exception as e:
            logging.error("Error extracting directed edges: %s", e)
            logging.debug("Problematic response: %s", response)
            input_data["directed_edges"] = None
            input_data["undirected_edges"] = None
        return input_data

    def _build_enhanced_prompt(self, input_data: dict[str, Any]) -> str:
        """æ„å»ºåŒ…å«æœç´¢ç»“æœçš„å¢å¼ºprompt"""
        base_prompt = self.prompt_template.format(
            premise=input_data["premise"],
            nodes=input_data["nodes"],
            edges=input_data["undirected_edges"],
            v_structures=input_data["v_structures"]
        )
        
        # âœ… ä¿®å¤ï¼šæ·»åŠ å¹¿æ³›æœç´¢ç»“æœ
        if input_data.get('_broad_search_summary'):
            base_prompt += f"\n\nDomain background context: {input_data['_broad_search_summary']}"
        
        # æ·»åŠ é’ˆå¯¹æ€§æœç´¢ç»“æœï¼ˆä¿®å¤keyåç§°ï¼‰
        if hasattr(self, '_current_search_summary') and self._current_search_summary:
            base_prompt += f"\n\nAdditional context for edge direction determination:\n{self._current_search_summary}"
        
        return base_prompt

    def process_batch(self, inputs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        logging.info("MeekRulesStage: Processing batch with %d samples.", len(inputs))
        for i, input_data in enumerate(inputs):
            try:
                self.process(input_data)
            except Exception as e:
                logging.error(f"Error processing sample {i}: {e}")
                input_data["directed_edges"] = None
                input_data["undirected_edges"] = None
        return inputs

class HypothesisEvaluationStage(Stage):
    """
    Stage for evaluating the hypothesis based on the directed edges.
    """
    prompt_template = Stage.prompts["hypothesis_evaluation"]

    def process(self, input_data: dict[str, Any]) -> dict[str, Any]:
        # 1. Validate inputs
        required_keys = {"premise", "nodes", "directed_edges", "hypothesis", "undirected_edges"}
        if not required_keys.issubset(input_data):
            raise ValueError(f"Hypothesis evaluation stage input data must contain: {', '.join(required_keys)}.")


        # 3. Build enhanced prompt (ä¸åŒ…å«é’ˆå¯¹æ€§æœç´¢ç»“æœ)
        prompt = self._build_enhanced_prompt(input_data)

        # 4. Send request to LLM
        logging.info("HypothesisEvaluationStage: Sending prompt to LLM.")
        response, usage = self.client.complete(prompt=prompt)

        # ğŸ†• æ·»åŠ LLMå“åº”æ—¥å¿—
        self._log_llm_response("HypothesisEvaluationStage", prompt, response, usage, "main")

        # 5. Unpack responses and update token usage
        try:
            hypothesis_label = extract_hypothesis_answer(answer=response)
            input_data["hypothesis_label"] = hypothesis_label
            
            logging.info(f"HypothesisEvaluationStage: Hypothesis evaluation result: {hypothesis_label}")
        except Exception as e:
            logging.error("Error extracting hypothesis_label: %s", e)
            logging.debug("Problematic response: %s", response)
            input_data["hypothesis_label"] = None
        return input_data

    def _build_enhanced_prompt(self, input_data: dict[str, Any]) -> str:
        """æ„å»ºpromptï¼ŒåªåŒ…å«å¹¿æ³›æœç´¢ç»“æœï¼Œä¸åŒ…å«é’ˆå¯¹æ€§æœç´¢"""
        base_prompt = self.prompt_template.format(
            premise=input_data["premise"],
            nodes=input_data["nodes"],
            directed_edges=input_data["directed_edges"],
            undirected_edges=input_data["undirected_edges"],
            hypothesis=input_data["hypothesis"]
        )
        
        # # âœ… ä¿ç•™ï¼šåªæ·»åŠ å¹¿æ³›æœç´¢ç»“æœï¼ˆæ¥è‡ªBroadRetrievalStageï¼‰
        # if input_data.get('_broad_search_summary'):
        #     base_prompt += f"\n\nDomain background context: {input_data['_broad_search_summary']}"
        
        # ğŸ†• åˆ é™¤ï¼šä¸å†æ·»åŠ é’ˆå¯¹æ€§æœç´¢ç»“æœ
        # if hasattr(self, '_current_search_summary') and self._current_search_summary:
        #     base_prompt += f"\n\nAdditional context for hypothesis evaluation:\n{self._current_search_summary}"
        
        return base_prompt

    def process_batch(self, inputs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        logging.info("HypothesisEvaluationStage: Processing batch with %d samples.", len(inputs))
        for i, input_data in enumerate(inputs):
            try:
                self.process(input_data)
            except Exception as e:
                logging.error(f"Error processing sample {i}: {e}")
                input_data["hypothesis_label"] = None
        return inputs


class KnowledgeGraphRetrievalStage(Stage):
    prompt_template = Stage.prompts["kg_search_queries"]

    def __init__(self, client, kg_client):
        super().__init__(client)
        self.kg_client = kg_client  # ä¼ å…¥WikidataClientå®ä¾‹

    def process(self, input_data: dict) -> dict:
        # 1. ç”Ÿæˆå®ä½“å¯¹æŸ¥è¯¢ï¼ˆåŸºäºå½“å‰å› æœå›¾èŠ‚ç‚¹ï¼‰
        prompt = self.prompt_template.format(
            premise=input_data["premise"],
            hypothesis=input_data["hypothesis"],
            nodes=input_data.get("nodes", []),
            directed_edges=input_data.get("directed_edges", []),
            undirected_edges=input_data.get("undirected_edges", [])
        )
        response, _ = self.client.complete(prompt=prompt)
        print("\n\n\nresponse: ", response)
        entity_pairs = self._parse_entity_pairs(response)  # è§£æå®ä½“å¯¹

        # 2. æ‰§è¡ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢ï¼ˆæŸ¥æ‰¾ä¸­ä»‹å’Œå…±åŒåŸå› ï¼‰
        kg_results = []
        for entity1, entity2 in entity_pairs:
            mediators = self.kg_client.find_mediators(entity1, entity2)
            common_causes = self.kg_client.find_common_causes(entity1, entity2)
            print("\n\n\nmediators: ", mediators)
            kg_results.append({
                "entity_pair": (entity1, entity2),
                "mediators": mediators,
                "common_causes": common_causes
            })
        input_data["kg_results"] = kg_results

        # 3. ç”¨ç»“æœå¢å¼ºå› æœå›¾
        enhance_prompt = Stage.prompts["kg_search_enhancement"].format(
            nodes=input_data["nodes"],
            directed_edges=input_data["directed_edges"],
            undirected_edges=input_data["undirected_edges"],
            kg_results=kg_results
        )
        enhance_response, _ = self.client.complete(prompt=enhance_prompt)
        enhanced_structure = extract_initial_construct_json(enhance_response)

        # åˆå¹¶å¢å¼ºåçš„èŠ‚ç‚¹å’Œè¾¹
        input_data["nodes"].extend(enhanced_structure.get("nodes", []))
        input_data["directed_edges"].extend(enhanced_structure.get("directed_edges", []))
        return input_data

    def _parse_entity_pairs(self, response: str) -> List[tuple]:
        """è§£æLLMç”Ÿæˆçš„å®ä½“å¯¹"""
        try:
            data = json.loads(response.replace("```json", "").replace("```", ""))
            return [tuple(pair) for pair in data.get("entity_pairs", [])]
        except json.JSONDecodeError:
            return []



class RAGEnhancementStage(Stage):
    """åŸºäºåœ¨çº¿RAGçš„å› æœå›¾å¢å¼ºé˜¶æ®µ"""
    prompt_template = Stage.prompts["rag_enhancement"]

    def __init__(self, client, rag_client):
        super().__init__(client)
        self.rag_client = rag_client

    def process(self, input_data: dict) -> dict:
        # 1. ç”ŸæˆRAGæŸ¥è¯¢
        query = self._generate_rag_query(input_data)
        if not query:
            return input_data

        # 2. æ‰§è¡Œåœ¨çº¿RAGæœç´¢
        rag_results = self.rag_client.rag_search(query)
        input_data["rag_results"] = rag_results

        # 3. ç”¨RAGç»“æœå¢å¼ºå› æœå›¾
        if rag_results["rag_contexts"] or rag_results["causal_relations"]:
            enhanced_graph = self._enhance_graph(input_data, rag_results)
            input_data.update(enhanced_graph)

        return input_data

    def _generate_rag_query(self, input_data: dict) -> str:
        """ç”ŸæˆRAGæœç´¢æŸ¥è¯¢"""
        nodes = [node["label"] for node in input_data.get("nodes", [])]
        edges = [f"{e['from']}->{e['to']}" for e in input_data.get("directed_edges", [])]

        prompt = f"""ç”Ÿæˆä¸€ä¸ªæœç´¢æŸ¥è¯¢ï¼Œç”¨äºè·å–ä»¥ä¸‹å› æœå…³ç³»çš„èƒŒæ™¯ä¿¡æ¯ï¼š
        èŠ‚ç‚¹: {nodes}
        å…³ç³»: {edges}
        å‰æ: {input_data.get('premise')}
        å‡è®¾: {input_data.get('hypothesis')}
        è¾“å‡ºç®€æ´çš„æŸ¥è¯¢è¯­å¥ï¼ˆä¸è¶…è¿‡20å­—ï¼‰"""

        response, _ = self.client.complete(prompt=prompt)
        return response.strip()

    def _enhance_graph(self, input_data: dict, rag_results: dict) -> dict:
        """ä½¿ç”¨RAGç»“æœå¢å¼ºå› æœå›¾"""
        print("\n\n\nPrompt Template: ", self.prompt_template)
        prompt = self.prompt_template.format(
            nodes=input_data.get("nodes", []),
            edges=input_data.get("directed_edges", []),
            rag_contexts=rag_results.get("rag_contexts", []),
            causal_relations=rag_results.get("causal_relations", [])
        )

        response, _ = self.client.complete(prompt=prompt)
        return self._parse_enhanced_graph(response)

    def _parse_enhanced_graph(self, response: str) -> dict:
        """è§£æå¢å¼ºåçš„å› æœå›¾"""
        import re
        import json
        json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                self.logger.error("Failed to parse enhanced graph JSON")
        return {}